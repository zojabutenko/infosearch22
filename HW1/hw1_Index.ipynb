{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S61LOApQNjWa"
      },
      "source": [
        "# ДЗ 1 Индекс"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC3vaLZmNjWu"
      },
      "source": [
        "## __Задача__: \n",
        "\n",
        "**Data:** Коллекция субтитров сезонов Друзей. Одна серия - один документ.\n",
        "\n",
        "**To do:** \n",
        "\n",
        "**1 Создайте обратный индекс этой базы в двух форматах: словарь и матрица.**\n",
        "\n",
        "Компоненты вашей реализации:\n",
        "    - Функция препроцессинга данных. Включите туда лемматизацию, приведение к одному регистру, удаление пунктуации и стоп-слов.\n",
        "    - Функция индексирования данных. На выходе создает обратный индекс.\n",
        "\n",
        "**2 С помощью обратного индекса в каждом формате посчитайте:** \n",
        "\n",
        "\n",
        "a) какое слово является самым частотным\n",
        "\n",
        "b) какое самым редким\n",
        "\n",
        "c) какой набор слов есть во всех документах коллекции\n",
        "\n",
        "d) кто из главных героев статистически самый популярный (упонимается чаще всего)? Имена героев:\n",
        "- Моника, Мон\n",
        "- Рэйчел, Рейч\n",
        "- Чендлер, Чэндлер, Чен\n",
        "- Фиби, Фибс\n",
        "- Росс\n",
        "- Джоуи, Джои, Джо\n",
        "\n",
        "**На что направлены эти задачи:** \n",
        "1. Навык построения обратного индекса\n",
        "2. Навык работы с этим индексом\n",
        "\n",
        "[download_friends_corpus](https://yadi.sk/d/4wmU7R8JL-k_RA?w=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports"
      ],
      "metadata": {
        "id": "uMzbHidRR-zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVvdE00BPfsO",
        "outputId": "b4ca0543-d5b3-4f0e-e7f1-4338afd75cdc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJZEVaF2Retf",
        "outputId": "404dac4b-a112-47df-88f5-083d9ce6814a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=ca52e4c5944c968355e2277f5b933bb66bf65d672f5e15d4405169409fff75cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "import re\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ADbHyNIzRZQC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting data"
      ],
      "metadata": {
        "id": "Kkxpz_w1SBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/infosearch22/hw1/friends-data'"
      ],
      "metadata": {
        "id": "VZJD5IxyPt1s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for name in files:\n",
        "        with open(os.path.join(root, name)) as f:\n",
        "          text = f.read()\n",
        "          corpus.append(text)"
      ],
      "metadata": {
        "id": "Wge0amFoPzsj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing"
      ],
      "metadata": {
        "id": "xNJ9-3K7SC5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words(\"russian\")#stop words\n",
        "morph = pymorphy2.MorphAnalyzer() #lemmatizer\n",
        "\n",
        "TOKEN_RE = re.compile(r'[а-яё]+')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl5a_1_ZRpaZ",
        "outputId": "31145507-8c67-49fa-eb7d-987d1109d4f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = ''.join(text.split('\\n')[:-4]) # remove metainfo\n",
        "    text = text.lower() #lower case\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) #punktuation\n",
        "    drt_tokens = text.split() # creating a list of tokens\n",
        "    tokens = [] # empty list for proceed tokens\n",
        "    for i in drt_tokens:\n",
        "      if i.isalpha() and i not in stopwords:\n",
        "        i = morph.normal_forms(i.strip())[0]\n",
        "        tokens.append(i)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "nQXhu3I0Poex"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "number of texts (episodes)"
      ],
      "metadata": {
        "id": "KaIboXrkSEmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# в коллекции должно быть 165 файлов\n",
        "\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C90KNs7aR7MP",
        "outputId": "1ff04b59-a311-4945-88ba-9d1383c3cce8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prep_texts = [preprocess(x) for x in corpus]"
      ],
      "metadata": {
        "id": "TOO9hBelRln8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prep_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz6OnPmeSINs",
        "outputId": "5e93481a-559d-45e5-c39e-cbd55822b4e3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "165"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the results of preprocessing on the first text, first 10 tokens:"
      ],
      "metadata": {
        "id": "Mu4vaZ8mSmK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prep_texts[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch6F8h_GSgY6",
        "outputId": "038c7df9-c9d3-4ff0-ff97-9a6f14386ce5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['фибиести',\n",
              " 'всемогущий',\n",
              " 'денья',\n",
              " 'пожелать',\n",
              " 'мир',\n",
              " 'весь',\n",
              " 'мирей',\n",
              " 'голодая',\n",
              " 'беречь',\n",
              " 'тропический']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of names:\n",
        "\n",
        "names = [['Моника', 'Мон'],\n",
        "         ['Рэйчел', 'Рейч', 'Рэйч'],\n",
        "         ['Чендлер', 'Чэндлер', 'Чен'],\n",
        "         ['Фиби', 'Фибс'],\n",
        "         ['Росс'],\n",
        "         ['Джоуи', 'Джои', 'Джо']]\n",
        "\n",
        "# get lowercase:\n",
        "\n",
        "NAMES = []\n",
        "for n in names:\n",
        "  l = []\n",
        "  for nn in n:\n",
        "    l.append(nn.lower())\n",
        "  NAMES.append(l)\n",
        "\n",
        "NAMES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a-2xAQS1HR",
        "outputId": "9c125b36-a236-4e47-d7ee-bc41166f48a2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['моника', 'мон'],\n",
              " ['рэйчел', 'рейч', 'рэйч'],\n",
              " ['чендлер', 'чэндлер', 'чен'],\n",
              " ['фиби', 'фибс'],\n",
              " ['росс'],\n",
              " ['джоуи', 'джои', 'джо']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_case(prep_texts):  \n",
        "  vectorizer = CountVectorizer(analyzer='word')\n",
        "  X = vectorizer.fit_transform(prep_texts)\n",
        "\n",
        "  features = vectorizer.get_feature_names()\n",
        "  matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
        "  most_freq = features[np.argmax(matrix_freq)]\n",
        "  less_freq = features[np.argmin(matrix_freq)]\n",
        "\n",
        "  full = np.apply_along_axis(lambda x: 0 not in x, 0, X.toarray())\n",
        "  id = np.where(full)[0]\n",
        "  every_word = [features[i] for i in id]\n",
        "  names_ = {}\n",
        "\n",
        "  for char in NAMES:\n",
        "      count = 0\n",
        "      for name in char:\n",
        "          index = vectorizer.vocabulary_.get(name.lower())\n",
        "          if index:\n",
        "              count += X.T[index].sum()\n",
        "      names_[char[0]] = 0\n",
        "      names_[char[0]] += count\n",
        "\n",
        "  freq_char = sorted(names_.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "  return most_freq, less_freq, every_word, freq_char"
      ],
      "metadata": {
        "id": "TYhZtRXuS_ql"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dictionary(texts):\n",
        "    dictionary = {}\n",
        "    for keys, values in texts.items():\n",
        "        for i in values:\n",
        "            rvr = dictionary.setdefault(i, {})\n",
        "            rvr[keys] = rvr.get(keys, 0) + 1\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "XTu-aXu8Tdt4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_case(d):\n",
        "    rotated_dic = create_dictionary(d)\n",
        "\n",
        "    counts = {} \n",
        "    for word, documents in rotated_dic.items():\n",
        "        for k, v in documents.items():\n",
        "            counts[word] = counts.get(word, 0) + v\n",
        "\n",
        "    sorted_dict = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
        "    most_freq_d = list(sorted_dict.items())[0][0]\n",
        "    less_freq_d = list(sorted_dict.items())[-1][0]\n",
        "    every_word_d = [word for word, documents in rotated_dic.items() if len(documents) == 165]\n",
        "\n",
        "    chars = {}\n",
        "    for c in NAMES:\n",
        "        counter = 0\n",
        "        for name in c:\n",
        "            try:\n",
        "                counter += counts[name]\n",
        "            except KeyError:\n",
        "                pass\n",
        "        chars[c[0]] = 0\n",
        "        chars[c[0]] += counter\n",
        "    freq_char_d = sorted(chars.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "    return most_freq_d, less_freq_d, every_word_d, freq_char_d"
      ],
      "metadata": {
        "id": "MQRvXppQTnSU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {}\n",
        "i = 1\n",
        "\n",
        "prep_corp = [' '.join(x) for x in prep_texts]\n",
        "\n",
        "for p in range(len(prep_corp)):\n",
        "  d[p + 1] = prep_corp[p]\n",
        "\n",
        "\n",
        "most_freq, less_freq, every_doc, most_freq_char = matrix_case(prep_corp)\n",
        "most_freq_d, least_freq_d, every_doc_d, most_freq_char_d = dict_case(d)\n",
        "\n",
        "print('\\n===== RESULTS FOR MATRIX APPROACH: =====\\n')\n",
        "print('Most frequent word in the matrix: {}'.format(most_freq))\n",
        "print('Least frequent word in the matrix: {}'.format(less_freq))\n",
        "print('Word that appears in every document: {}'.format(every_doc))\n",
        "print('Most frequently mentioned character: {}'.format(most_freq_char.capitalize()))\n",
        "print('\\n===== RESULTS FOR DICTIONARY APPROACH: =====\\n')\n",
        "print('Most frequent word in the matrix: {}'.format(most_freq_d))\n",
        "print('Least frequent word in the matrix: {}'.format(least_freq_d))\n",
        "print('Word that appears in every document: {}'.format(every_doc_d))\n",
        "print('Most frequently mentioned character: {}'.format(most_freq_char_d.capitalize()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRZgQag2VPn9",
        "outputId": "046def7d-7931-44aa-f702-8b9a7b9494bd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== RESULTS FOR MATRIX APPROACH: =====\n",
            "\n",
            "Most frequent word in the matrix: это\n",
            "Least frequent word in the matrix: after\n",
            "Word that appears in every document: ['знать', 'это']\n",
            "Most frequently mentioned character: Росс\n",
            "\n",
            "===== RESULTS FOR DICTIONARY APPROACH: =====\n",
            "\n",
            "Most frequent word in the matrix:  \n",
            "Least frequent word in the matrix: x\n",
            "Word that appears in every document: ['ф', 'и', 'б', 'е', 'с', 'т', ' ', 'в', 'м', 'о', 'г', 'у', 'щ', 'й', 'д', 'н', 'ь', 'я', 'п', 'ж', 'л', 'а', 'р', 'ч', 'к', 'ш', 'з', 'ы', 'ё', 'э', 'х', 'ю', 'ц']\n",
            "Most frequently mentioned character: Моника\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}